{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning intro\n",
    "\n",
    "ML is a method of data analysis that automates model building\n",
    "\n",
    "computers can use algorithms to iteratively learn from data and find hidden insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural netwoks\n",
    "\n",
    "Model biological neuron system mathematically, can perform more tasks like image classification\n",
    "\n",
    "## Deep learning\n",
    "\n",
    "> Neural network with > 1 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "Use <b>labelled</b> examples\n",
    "\n",
    "eg: spam vs legit email\n",
    "\n",
    "In neural networks, computer looks at historical data to predict likely\n",
    "future events, corrects its own error by checking correct output labels\n",
    "that user has provided\n",
    "\n",
    "data acquisition -> cleaning\n",
    "\n",
    " -> training on training data + building -> testing on validation data\n",
    "\n",
    " -> test data\n",
    " \n",
    "-> model testing -> deployment\n",
    "\n",
    "2. Beware of overfitting & underfitting\n",
    "\n",
    "> overfitting: too much training on the noise of training data (/\\/\\ instead of /---)\n",
    "\n",
    "If we overfit on training data then the error on test data would suddenly\n",
    "increase as we train on more epochs (more training data)\n",
    "\n",
    "> underfitting: not capturing underlying trend of data, low variance high bias\n",
    "\n",
    "## metrics to judge our classification problems\n",
    "\n",
    "P = TP + FN\n",
    "\n",
    "N = TN + FP\n",
    "\n",
    "1. accuracy: TP/(P+N)\n",
    "\n",
    "<i>drawback: problem occurs when we have overfitted/underfitted model which only\n",
    "'predicts' one class (bias) so if we have <b>unbalanced</b> test data just from\n",
    "that class our accuracy will be very high</i>\n",
    "\n",
    "2. recall: TP/(TP+FN)\n",
    "\n",
    "3. precision: TP/(TP+FP)\n",
    "\n",
    "4. F1 score is a blend of recall and precision to find an optimal balance\n",
    "between the 2 metrics\n",
    "\n",
    "> 2 * precision * recall(precision+recall)\n",
    "\n",
    "for disease diagnostics, we prefer to train our models such that false positives\n",
    "are preferred more than false negatives\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluating performance for regression\n",
    "\n",
    " 1. mean of absolute errors\n",
    " `(E|x-mean|)/n`\n",
    " > does not fit all shapes of data\n",
    " 2. mean of squared errors\n",
    " `(E|x-mean|^2)/n`\n",
    " > punishes larger outliers more, but the dimensions are sqared as well so we go for RMSE\n",
    " 2. rootmean of squared errors\n",
    " `(sqrdroot(E|x-mean|^2)/n)`\n",
    " > evaluate by context, domain knowledge of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "> generalized matrices (1D/2D/3D...)\n",
    "### naming convention of tensors\n",
    "> single value: scalar\n",
    "> 1D: vector\n",
    "> 2D: matrix\n",
    "> 3D+: tensor\n",
    "\n",
    "it is easy to arrange our data in tensors, esp with image sets where image layers are another dimension\n",
    "\n",
    "tensor contains multidemnsional data of single data type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## interpreting image data\n",
    "\n",
    "1. the image is interpreted as 2d matrix of binary values\n",
    "\n",
    "2. the rows are flattened out into a single input array\n",
    "\n",
    "3. this array is fed into our neural network that contains hidden layers\n",
    " > the NN receives each digit of the long input array as individual neuron input\n",
    "\n",
    "4. final output is derived from 10 condensed neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayerperceptron\n",
    "\n",
    "it is a type of feedfoward ANN(artificial neural network) that provides output WITHOUT feedback (no back propagation to previous neuron layers). It uses:\n",
    "1. linear input transformation (weights multiplied to input data and added to bias) y=E(wx)+b (same for SLP)\n",
    "1. loss function (which is optimized to increase accuracy)\n",
    "2. activation function (activates or outputs some info if this neuron recieves an input passing some threshold)\n",
    "\n",
    "has more than 1 layer (single layer perceptron), which are 'hidden' between input and final output\n",
    "\n",
    "with every iteration of input sampling, perceptron adjusts its weights \n",
    "\n",
    "## NOTE\n",
    "\n",
    "more hidden layers cause delays in training time and may not be required for some classification problems (since they add complexity to output generation)\n",
    "\n",
    "equivalent to writing:\n",
    "\n",
    "`mlp = MLPClassifier(hidden_layer_sizes=(100, 50), learning_rate_init=0.001, max_iter=500, random_state=42)`\n",
    "\n",
    "The learning rate specifies how much the parameters will be optimized in the opposite direction of the loss function's gradient (size of the steps  in optimizing the weights in each iteration, generally good to keep small)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "they are basically kernels (small matrices) that are multiplied to similar size matrix of image (convolution) and the final sum of that matrix replaces the first value in input matrix, then the stride continues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
